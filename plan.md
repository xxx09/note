计划：
定义强化学习目前的目标：
讨论一下分工的问题；
平台，迁移到操作上
针对不同的机器人能够形成快速学习，
不同模态之间进行切换；
适应不同的机器人；
如何优化腿的使用，不同腿型的步态设计；

安全方面相关的步态；如何迭代；跌倒爬起，跌倒的安全措施；

1. 全地形的行走；泽霖，0.6m/s;0.8m/s; 适应性；
2. 拟态的动作，人类学习：跳舞，坐，站；实际应用场景的规划； 小周博拟态的任务；
3. 高速的跑步

性能上讲：如何节能；
长期而言：安全方面考虑；

计划：
1. 平台的目标； 步态设计，动捕系统、仿真、视频
步态的training
sim2real的工作
2. 具体的目标；
1.平台上具体的目标；
提取步态；retarget的方式；小周博做的比较多；大概写一下；主要做review用；
acheivement；
起身--

训练的平台：泽霖的isaaclab，
基于gym的训练，
leggedlab可控一点；往这里面去迁移；前期迁移成本很高；follow算法用gym

ppo的算法训练；ppo_amp也加入，但没训练好，训练效果是差不多的；起身+跑步；

amp+任何方法，一系列奖励函数，reward，Amp为风格的奖励，通过单一的网络去训练，很多为分层的：预训练+后训练；
算法的方案；
包括动捕的数据，起身，跑步；
rl的库，rl_games, rsl-rl, skrl,
仿真里面做复现；
1.全地形 2.跑步跳舞 3.从腿到手臂的应用；

AMP要试试，卡在数据，验证数据， dvh->到我们， phc的retarget方案；

数据，每一帧数据都是合理的，关注接触，CMU数据，可能有坡度；

Mira定义具体是哪一款；

今年下半年，

全地形+AMP做完；mirax，10月底，科技月（研究成本展示）；微微屈膝；

算法验证无所谓；

GMT；

AMP的命令跟随，也需要很久，数据；

AMP不确定性更高的，抠细节，一定要扣；

数据处理，

宇树的开源库，看一下； 

模仿学习，跳舞之类的，也有的；做动作的话，可能更新鲜点；技术框架；amp，ase，asap，调研一下;

表演的话；数据有共性的问题；

走路，优化，

rsp, submodule, 何博的权限； 共享内存，锁指针的方式，框架比较复杂；lcm的数据；

# 人形机器人运动控制工作计划
和人形机器人强化学习算法团队一起完成以下目标
1. 全地形行走
- 基于mirax机器人硬件，实现mirax在以下地形的行走：
- 1.1 平地
    - 地毯地面、大理石地面、塑胶地面等各类平地的行走
- 1.2 不平整地面
    - 草地、石子地等不平整地面
- 1.3 斜坡
    - 实验室斜坡，户外小山坡等斜坡地面的行走
- 1.4 台阶
    - 实验室台阶，实地楼梯台阶

2. 数据重定向
- 将动捕、视频等数据retarget到mirax等机器人上
- 校验动作的合理性，检查数据的质量
- 可用于后续的行走姿态优化及全身动作模仿

3. 行走姿态的优化
- 基于AMP、ASE等算法的拟人步态行走，让机器人行走时更像人

4. 全身动作模仿
- 实现一套完整的动作流程；比如跳一段舞蹈，或者打一套拳
- 技术框架调研选择
- 整套动作的训练pipeline：基于deepmimic或其他算法进行实现； ASAP等；

5. 起身
- 5.1 实现人形机器人从平躺到站立的功能
- 5.2 后续需要实现从任意姿态到站立的功能

6. 跑步
- 6.1 实现人形机器跑步的功能


tensor([-0.0147, -0.1572,  0.8222,  ..., -0.1237, -0.1798, -0.1699],
       device='cuda:0')
- 
