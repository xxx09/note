# PPO vs MHPPO 算法对比表格

## 核心差异对比

| 对比维度                | PPO                          | MHPPO                                    |
|------------------------|------------------------------|------------------------------------------|
| 奖励函数支持            | 单一标量奖励                  | 多个奖励函数向量                          |
| 奖励存储格式            | `shape=(1,)`                 | `shape=(num_rew_fn,)`                    |
| 价值函数输出            | 单一价值估计                  | 每个奖励函数对应价值估计                   |
| Critic输出维度          | 固定单维                     | 动态设置为`num_rew_fn`                   |
| 网络架构               | 标准PPOActor/PPOCritic        | 可选PhaseAwareActor/Critic               |
| 观测存储               | 仅当前观测                    | 当前+下一步观测                          |
| 正则化方法             | 无额外正则化                  | L2C2平滑正则化                           |
| 优势函数计算           | 直接标准化                    | 跨奖励求和后标准化                        |
| 价值损失计算           | `.mean()`                    | `.sum(dim=-1).mean()`                   |

## 详细技术规格

| 技术细节                | PPO实现                      | MHPPO实现                                |
|------------------------|------------------------------|------------------------------------------|
| 奖励累积方式            | `cur_reward_sum += rewards`   | `cur_reward_sum += rewards.sum(dim=-1)`  |
| Returns计算            | 标量GAE                      | 向量GAE，每个奖励独立计算                 |
| Advantages聚合         | 无需聚合                     | `tot_advantages.sum(dim=-1)`             |
| 存储键注册             | 基础obs键                    | 基础obs键 + next_obs键                   |
| L2C2插值              | 不适用                       | `u_obs = obs + u*(next_obs-obs)`         |
| 相位嵌入支持           | 不支持                       | 支持相位感知网络架构                      |
| 损失函数组成           | Value + Surrogate + Entropy   | Value + Surrogate + Entropy + L2C2       |

## 配置参数差异

| 配置项                 | PPO                          | MHPPO                                    |
|------------------------|------------------------------|------------------------------------------|
| `num_rew_fn`           | 固定为1                      | 从环境动态获取                            |
| `phase_embed`          | 不支持                       | 可选配置相位嵌入                          |
| `l2c2.enable`          | 不支持                       | 可选启用L2C2正则化                       |
| `l2c2.lambda_value`    | 不适用                       | L2C2价值函数权重                         |
| `l2c2.lambda_policy`   | 不适用                       | L2C2策略函数权重                         |

## 算法适用场景

| 应用场景               | PPO                          | MHPPO                                    |
|------------------------|------------------------------|------------------------------------------|
| 单目标任务             | ✅ 适合                      | ✅ 兼容但过度工程化                       |
| 多目标任务             | ❌ 需要手动权重组合奖励        | ✅ 原生支持                              |
| 相位敏感任务           | ❌ 不支持                     | ✅ 支持相位感知                          |
| 连续控制任务           | ✅ 标准适用                   | ✅ 更好的平滑性约束                       |
| 人形机器人控制         | ⚠️ 需要调整                   | ✅ 专门设计                              |
| 计算资源要求           | 💚 轻量级                     | 🔶 中等（多头计算开销）                    |

## 性能特点对比

| 性能指标               | PPO                          | MHPPO                                    |
|------------------------|------------------------------|------------------------------------------|
| 训练稳定性             | 🟢 稳定                      | 🟢 稳定（L2C2增强）                       |
| 收敛速度               | 🟡 标准                      | 🟡 略慢（多头计算）                       |
| 样本效率               | 🟡 标准                      | 🟢 更好（多奖励信息）                     |
| 内存占用               | 🟢 低                        | 🟡 中等（存储next_obs）                   |
| 计算复杂度             | 🟢 O(1)奖励                  | 🟡 O(num_rew_fn)奖励                     |
| 可解释性               | 🟢 简单直观                   | 🟡 复杂但信息丰富                        |